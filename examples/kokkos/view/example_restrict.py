"""
Using the :code:`__restrict__` qualifier to hint the compiler that pointers do not
alias can bring significant gains, such as:

1. Use the ``.CONSTANT`` modifier for the load instruction when the data is read-only.
2. Reduce redundant memory loads by assuming no aliasing between pointers.

See :cite:label:`nvidia-cuda-restrict-pointers` for more details.

However, depending on the context in which the :code:`__restrict__` qualifier is used, compilers
may not be able to take advantage of it.

This example examines the SASS code generated by:

.. literalinclude:: ../../../../../examples/kokkos/view/example_restrict.cpp
    :language: c++
    :start-at: #define OPERATION
    :end-at: dst[index] +=

for the following implementations:

1. :py:attr:`Method.GLOBAL_KERNEL`, see :py:meth:`TestSASS.test_global_kernel`
2. :py:attr:`Method.RESTRICT_RECAST_LAMBDA`, see :py:meth:`TestSASS.test_restrict_recast_lambda`
3. :py:attr:`Method.RESTRICT_RECAST_LOCAL`, see :py:meth:`TestSASS.test_restrict_recast_local`
4. :py:attr:`Method.RESTRICT_ACCESSOR`, see :py:meth:`TestSASS.test_restrict_accessor`
5. :py:attr:`Method.RESTRICT_MEMBER`, see :py:meth:`TestSASS.test_restrict_member`
6. :py:attr:`Method.RESTRICT_VIEW_MEMORY_TRAIT`, see :py:meth:`TestSASS.test_restrict_view_memory_trait`
7. :py:attr:`Method.LDG_ACCESSOR`, see :py:meth:`TestSASS.test_ldg_accessor`

These implementations either generate a single sequence of loads/operation/store
when the compiler figures out pointers do not alias (:py:meth:`TestSASS.match_single`),
or two sequences otherwise (:py:meth:`TestSASS.match_repeated`).
When it generates two sequences, the memory traffic is increased significantly, as shown in
:py:class:`TestNCU`.

Strategies preventing pointer aliasing
--------------------------------------

The following strategies map to a single sequence, but each has a drawback:

#. :py:attr:`Method.GLOBAL_KERNEL`

   .. caution::

        Writing a :code:`__global__` kernel with :code:`__restrict__` qualified arguments is not portable.

#. :py:attr:`Method.RESTRICT_RECAST_LAMBDA`

   .. caution::

        Requires significant boilerplate code to be added.
        Hinders readability significantly.

#. :py:attr:`Method.LDG_ACCESSOR`

   .. caution::

        Unsuitable for non-constant data buffers.
        Not directly related to pointer aliasing, but may have similar effects.

   .. note::

        According to :cite:label:`nvidia-cuda-ldg`, :code:`__ldg` (or any other cache strategy)
        *supports all C++ fundamental types, CUDA vector types (except x3 components), and extended floating-point types*.
        However, as of CUDA 13.1, it seems to only support up to :code:`double2`, *i.e.* up to
        128-bit size objects. This seems contradictory with
        :py:meth:`reprospect.test.features.Memory.max_transaction_size` that states 256-bit size objects
        are supported as of :py:attr:`reprospect.tools.architecture.NVIDIAFamily.BLACKWELL` and CUDA 13.

``.CONSTANT`` load
------------------

To ensure that a global load with ``.CONSTANT`` is used, use either of:

#. :py:attr:`Method.GLOBAL_KERNEL`
#. :py:attr:`Method.LDG_ACCESSOR`

Strategies that fail at preventing pointer aliasing
---------------------------------------------------

The following strategies cannot be used to avoid pointer aliasing:

#. :py:attr:`Method.RESTRICT_RECAST_LOCAL`

   It is not working across all compilers.

#. :py:attr:`Method.RESTRICT_ACCESSOR`

   Writing a custom accessor, *e.g.* for `std::mdspan`_,
   does not work.

   This is the strategy employed in `PR 8151 <https://github.com/kokkos/kokkos/pull/8151>`_.

#. :py:attr:`Method.RESTRICT_MEMBER`

   This strategy is also ineffective, as already noted in:

   - https://discuss.pytorch.org/t/usage-of-restrict-in-cuda/150395
   - https://forums.developer.nvidia.com/t/restrict-seems-to-be-ignored-for-base-pointers-in-structs-having-base-pointers-with-restrict-as-kernel-arguments-directly-works-as-expected/154020

#. :py:attr:`Method.RESTRICT_VIEW_MEMORY_TRAIT`

   The implementation is similar to :py:attr:`Method.RESTRICT_ACCESSOR`.
"""

import logging
import pathlib
import re
import sys
import typing

import pytest

from reprospect.test import CMakeAwareTestCase, environment
from reprospect.test.sass.composite import (
    any_of,
    instruction_is,
    instructions_contain,
    interleaved_instructions_are,
)
from reprospect.test.sass.controlflow.block import BasicBlockMatcher
from reprospect.test.sass.instruction import (
    LoadGlobalMatcher,
    OpcodeModsWithOperandsMatcher,
    StoreGlobalMatcher,
)
from reprospect.test.sass.instruction.integer import (
    IntAdd3Matcher,
    LEAMatcher,
)
from reprospect.test.sass.instruction.register import Register
from reprospect.test.sass.matchers.add_int32 import AddInt32Matcher
from reprospect.tools import ncu
from reprospect.tools.binaries.cuobjdump import CuObjDump
from reprospect.tools.sass.controlflow import ControlFlow, Graph
from reprospect.tools.sass.decode import Decoder
from reprospect.utils import detect

if sys.version_info >= (3, 12):
    from typing import override
else:
    from typing_extensions import override

if sys.version_info >= (3, 11):
    from enum import StrEnum
else:
    from backports.strenum.strenum import StrEnum


class Method(StrEnum):
    GLOBAL_KERNEL = 'global_kernel'
    """
    Use a :code:`__global__` kernel with :code:`__restrict__`
    qualified pointer arguments.
    """

    RESTRICT_RECAST_LAMBDA = 'FunctorRestrictRecastLambda'
    """
    Recast pointers to :code:`__restrict__` qualified pointers through an intermediate lambda
    within the call operator.
    """

    RESTRICT_RECAST_LOCAL = 'FunctorRestrictRecastLocal'
    """
    Recast pointers to :code:`__restrict__` qualified pointers through intermediate local variables
    within the call operator.
    """

    RESTRICT_ACCESSOR = 'FunctorRestrictAccessor'
    """
    Use a *restrict accessor* that declares the reference type as :code:`__restrict__`.
    """

    RESTRICT_MEMBER = 'FunctorRestrictMember'
    """
    Declare pointer members as :code:`__restrict__`.
    """

    RESTRICT_VIEW_MEMORY_TRAIT = 'FunctorRestrictViewMemoryTrait'
    """
    Use the :code:`Kokkos::Restrict` memory trait.
    """

    LDG_ACCESSOR = 'FunctorLDGAccessor'
    """
    Use a *ldg accessor* that uses :code:`__ldg` to enforce read-only L1/Tex cache load.

    References:

    * :cite:`nvidia-cuda-ldg`
    """

class TestRestrict(CMakeAwareTestCase):
    @classmethod
    @override
    def get_target_name(cls) -> str:
        return 'examples_kokkos_view_restrict'

    SCALAR_TYPE: typing.Final[str] = 'int'

    SIGNATURE_TEMPLATE: typing.Final[str] = (
        r'void Kokkos::Impl::cuda_parallel_launch_local_memory<'
        r'Kokkos::Impl::ParallelFor<reprospect::examples::kokkos::view::{functor}<{scalar}>, '
        r'Kokkos::RangePolicy<>, Kokkos::Cuda>>'
    )
    """Signature template when :code:`Kokkos::parallel_for` is used."""

    KOKKOS_TOOLS_NVTX_CONNECTOR_LIB = environment.EnvironmentField(converter=pathlib.Path)
    """Used in :py:meth:`TestNCU.report`."""

    @property
    def cubin(self) -> pathlib.Path:
        return self.cwd / f'{self.get_target_name()}.1.{self.arch.as_sm}.cubin'

    @pytest.fixture(scope='class')
    def signature(self) -> dict[Method, str]:
        signature: dict[Method, str] = {}

        signature[Method.GLOBAL_KERNEL] = rf'void reprospect::examples::kokkos::view::{Method.GLOBAL_KERNEL.value}<{self.SCALAR_TYPE}>'

        for method in Method:
            if method != Method.GLOBAL_KERNEL:
                signature[method] = self.SIGNATURE_TEMPLATE.format(functor=method.value, scalar=self.SCALAR_TYPE)

        return signature

class TestSASS(TestRestrict):
    """
    Binary analysis.
    """
    @pytest.fixture(scope='class')
    def cuobjdump(self) -> CuObjDump:
        return CuObjDump.extract(
            file=self.executable,
            arch=self.arch,
            sass=True, cwd=self.cwd,
            cubin=self.cubin.name,
            demangler=self.demangler,
        )[0]

    @pytest.fixture(scope='class')
    def decoder(self, signature: dict[Method, str], cuobjdump: CuObjDump) -> dict[Method, Decoder]:
        decoder: dict[Method, Decoder] = {}
        for method, sig in signature.items():
            matcher = re.compile(pattern=sig)
            [sig] = (sig for sig in cuobjdump.functions if matcher.search(sig) is not None)
            decoder[method] = Decoder(code=cuobjdump.functions[sig].code)
        return decoder

    def match_single(self, *, readonly: bool, cfg: Graph) -> bool:
        """
        Match a single loads/add/store sequence, such as::

            LDG.E.CONSTANT R2, desc[UR4][R2.64]
            LDG.E.CONSTANT R5, desc[UR4][R4.64]
            ...
            IADD3 R0, PT, PT, R2, R5, RZ
            IADD3 R9, PT, PT, R0, R0, RZ
            ...
            STG.E desc[UR4][R6.64], R9
        """
        # Find the block that has two global loads.
        matcher_ldg = instructions_contain(interleaved_instructions_are(
            LoadGlobalMatcher(arch=self.arch, size=32, readonly=readonly),
            LoadGlobalMatcher(arch=self.arch, size=32, readonly=readonly),
        ))
        if (value := BasicBlockMatcher(matcher_ldg).match(cfg=cfg)) is None:
            return False
        block, matched_ldg = value

        # In this block, find the first global store.
        matcher_stg = instructions_contain(StoreGlobalMatcher(arch=self.arch, size=32))
        if (matched_stg := matcher_stg.match(instructions=block.instructions[matcher_ldg.next_index:])) is None:
            return False

        # In between the loads and store, find instructions that perform the 2 additions.
        matcher_add = instructions_contain(any_of(
            interleaved_instructions_are(
                AddInt32Matcher(arch=self.arch, src_a=matched_ldg[0].operands[0], src_b=matched_ldg[1].operands[0]),
                any_of(
                    IntAdd3Matcher(arch=self.arch, dst=matched_stg[0].operands[1]),
                    OpcodeModsWithOperandsMatcher(opcode='IMAD', modifiers=('SHL', 'U32'), operands=(
                        matched_stg[0].operands[1],
                        Register.REG,
                        '0x2', 'RZ',
                    )),
                    OpcodeModsWithOperandsMatcher(opcode='SHF', modifiers=('L', 'U32'), operands=(
                        matched_stg[0].operands[1],
                        Register.REG,
                        '0x1', 'RZ',
                    )),
                ),
            ),
            interleaved_instructions_are(
                LEAMatcher(shift='0x1', index=matched_ldg[0].operands[0], base=matched_ldg[1].operands[0]),
                AddInt32Matcher(arch=self.arch),
            ),
            interleaved_instructions_are(
                OpcodeModsWithOperandsMatcher(opcode='IMAD', operands=(
                    Register.REG,
                    matched_ldg[0].operands[0], '0x2', matched_ldg[1].operands[0],
                )),
                OpcodeModsWithOperandsMatcher(opcode='IMAD', modifiers=('IADD',), operands=(
                    Register.REG, Register.REG, '0x1', Register.REG,
                )),
            ),
        ))
        return matcher_add.match(instructions=block.instructions[matcher_ldg.next_index:matcher_ldg.next_index + matcher_stg.next_index]) is not None

    def match_repeated(self, cfg: Graph) -> bool:
        """
        Match two loads/add/store sequences, such as::

            LDG.E R0, desc[UR4][R2.64]
            LDG.E R17, desc[UR4][R4.64]
            ...
            IADD3 R17, PT, PT, R0, R17, RZ
            ...
            STG.E desc[UR4][R6.64], R17
            ...
            LDG.E R0, desc[UR4][R2.64]
            LDG.E R8, desc[UR4][R4.64]
            ...
            IADD3 R9, PT, PT, R17, R8, R0
            ...
            STG.E desc[UR4][R6.64], R9
        """
        # Find the first two loads.
        matcher_ldg = instructions_contain(interleaved_instructions_are(
            LoadGlobalMatcher(arch=self.arch, size=32, readonly=False),
            LoadGlobalMatcher(arch=self.arch, size=32, readonly=False),
        ))
        if (value := BasicBlockMatcher(matcher_ldg).match(cfg=cfg)) is None:
            return False
        block, matched_ldg_a = value
        offset = matcher_ldg.next_index

        # First addition.
        matcher_add_a = instructions_contain(AddInt32Matcher(
            arch=self.arch,
            src_a=matched_ldg_a[0].operands[0],
            src_b=matched_ldg_a[1].operands[0],
            swap=True,
        ))
        if (matched_add_a := matcher_add_a.match(instructions=block.instructions[offset:])) is None:
            return False
        offset += matcher_add_a.next_index

        # First store.
        matcher_stg_a = instructions_contain(instruction_is(
            StoreGlobalMatcher(arch=self.arch, size=32)).with_operand(index=1, operand=matched_add_a[0].operands[0]),
        )
        if (matched_stg_a := matcher_stg_a.match(instructions=block.instructions[offset:])) is None:
            return False
        offset += matcher_stg_a.next_index

        # Second time.
        if (matched_ldg_b := matcher_ldg.match(instructions=block.instructions[offset:])) is None:
            return False
        offset += matcher_ldg.next_index

        matcher_add_b = instructions_contain(instruction_is(IntAdd3Matcher(
            arch=self.arch, src_c=Register.REG,
        )).with_operand(matched_stg_a[0].operands[1])
          .with_operand(matched_ldg_b[1].operands[0])
          .with_operand(matched_ldg_b[0].operands[0]),
        )
        if (matched_add_b := matcher_add_b.match(instructions=block.instructions[offset:])) is None:
            return False
        offset += matcher_add_b.next_index

        matcher_stg = instructions_contain(instruction_is(StoreGlobalMatcher(
            arch=self.arch, size=32)).with_operand(index=1, operand=matched_add_b[0].operands[0]),
        )
        return matcher_stg.match(instructions=block.instructions[offset:]) is not None

    def test_global_kernel(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.GLOBAL_KERNEL`.

        It generates a single loads/add/store sequence,
        and always uses the ``.CONSTANT`` load path.
        """
        logging.info(decoder[Method.GLOBAL_KERNEL])
        cfg = ControlFlow.analyze(instructions=decoder[Method.GLOBAL_KERNEL].instructions)
        assert self.match_repeated(cfg=cfg) is False
        assert self.match_single(cfg=cfg, readonly=False) is False
        assert self.match_single(cfg=cfg, readonly=True) is True

    def test_restrict_recast_lambda(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.RESTRICT_RECAST_LAMBDA`.

        It generates a single loads/add/store sequence,
        but misses the ``.CONSTANT`` modifier for recent architectures.
        """
        logging.info(decoder[Method.RESTRICT_RECAST_LAMBDA])
        cfg = ControlFlow.analyze(instructions=decoder[Method.RESTRICT_RECAST_LAMBDA].instructions)
        readonly = self.arch.compute_capability.as_int in {70, 75, 86, 89, 90}
        assert self.match_repeated(cfg=cfg) is False
        assert self.match_single(cfg=cfg, readonly=False) == (not readonly)
        assert self.match_single(cfg=cfg, readonly=True) == readonly

    def test_restrict_recast_local(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.RESTRICT_RECAST_LOCAL`.

        It generates a single loads/add/store sequence,
        and always uses the ``.CONSTANT`` load path for ``nvcc``.
        It generates two loads/add/store sequences for ``clang``.
        """
        logging.info(decoder[Method.RESTRICT_RECAST_LOCAL])
        cfg = ControlFlow.analyze(instructions=decoder[Method.RESTRICT_RECAST_LOCAL].instructions)
        match self.toolchains['CUDA']['compiler']['id']:
            case 'NVIDIA':
                assert self.match_repeated(cfg=cfg) is False
                assert self.match_single(cfg=cfg, readonly=False) is False
                assert self.match_single(cfg=cfg, readonly=True) is True
            case 'Clang':
                assert self.match_repeated(cfg=cfg) is True
                assert self.match_single(cfg=cfg, readonly=False) is False
                assert self.match_single(cfg=cfg, readonly=True) is False

    def test_restrict_accessor(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.RESTRICT_ACCESSOR`.

        It generates two loads/add/store sequences.
        """
        logging.info(decoder[Method.RESTRICT_ACCESSOR])
        cfg = ControlFlow.analyze(instructions=decoder[Method.RESTRICT_ACCESSOR].instructions)
        assert self.match_repeated(cfg=cfg) is True
        assert self.match_single(cfg=cfg, readonly=False) is False
        assert self.match_single(cfg=cfg, readonly=True) is False

    def test_restrict_member(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.RESTRICT_MEMBER`.

        It generates two loads/add/store sequences.
        """
        logging.info(decoder[Method.RESTRICT_MEMBER])
        cfg = ControlFlow.analyze(instructions=decoder[Method.RESTRICT_MEMBER].instructions)
        assert self.match_repeated(cfg=cfg) is True
        assert self.match_single(cfg=cfg, readonly=False) is False
        assert self.match_single(cfg=cfg, readonly=True) is False

    def test_restrict_view_memory_trait(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.RESTRICT_VIEW_MEMORY_TRAIT`.

        It generates two loads/add/store sequences.
        """
        logging.info(decoder[Method.RESTRICT_VIEW_MEMORY_TRAIT])
        cfg = ControlFlow.analyze(instructions=decoder[Method.RESTRICT_VIEW_MEMORY_TRAIT].instructions)
        assert self.match_repeated(cfg=cfg) is True
        assert self.match_single(cfg=cfg, readonly=False) is False
        assert self.match_single(cfg=cfg, readonly=True) is False

    def test_ldg_accessor(self, decoder: dict[Method, Decoder]) -> None:
        """
        Test for :py:attr:`Method.LDG_ACCESSOR`.

        It generates a single loads/add/store sequence,
        and always uses the ``.CONSTANT`` load path.
        """
        logging.info(decoder[Method.LDG_ACCESSOR])
        cfg = ControlFlow.analyze(instructions=decoder[Method.LDG_ACCESSOR].instructions)
        assert self.match_repeated(cfg=cfg) is False
        assert self.match_single(cfg=cfg, readonly=False) is False
        assert self.match_single(cfg=cfg, readonly=True) is True

@pytest.mark.skipif(not detect.GPUDetector.count() > 0, reason='needs a GPU')
class TestNCU(TestRestrict):
    """
    Kernel profiling.
    """
    METRICS: tuple[ncu.Metric | ncu.MetricCorrelation, ...] = (
        ncu.L1TEXCache.GlobalLoad.Sectors.create(),
        ncu.L1TEXCache.GlobalLoad.SectorMisses.create(),
        ncu.L1TEXCache.GlobalStore.Sectors.create(),
        ncu.MetricCounter(name='lts__t_sectors_srcunit_tex_op_read_lookup_miss', subs=(ncu.MetricCounterRollUp.SUM,)),
    )

    NVTX_INCLUDES: typing.Final[tuple[str, ...]] = tuple(f'{x.name.lower()}/' for x in Method)

    ELEMENT_COUNT: typing.Final[int] = 128
    ELEMENT_SIZE: typing.Final[int] = 4
    SECTOR_SIZE: typing.Final[int] = 32

    SECTOR_COUNT_LOAD: typing.Final[int] = 2 * ELEMENT_COUNT * ELEMENT_SIZE // SECTOR_SIZE
    """
    Expected number of load sectors requested for the *single* sequence scenario.
    """

    SECTOR_COUNT_STORE: typing.Final[int] = ELEMENT_COUNT * ELEMENT_SIZE // SECTOR_SIZE
    """
    Expected number of store sectors for the *single* sequence scenario.
    """

    @pytest.fixture(scope='class')
    def report(self) -> ncu.Report:
        with ncu.Cacher() as cacher:
            command = ncu.Command(
                output=self.cwd / 'ncu',
                executable=self.executable,
                metrics=self.METRICS,
                nvtx_includes=self.NVTX_INCLUDES,
                args=(
                    f'--kokkos-tools-libs={self.KOKKOS_TOOLS_NVTX_CONNECTOR_LIB}',
                ),
            )
            cacher.run(
                command=command,
                cwd=self.cwd,
                retries=5,
            )
        return ncu.Report(command=command)

    @pytest.fixture(scope='class')
    def results(self, report: ncu.Report) -> ncu.ProfilingResults:
        results = report.extract_results_in_range(metrics=self.METRICS)
        assert len(results) == len(Method)
        return results

    @pytest.mark.parametrize('method', Method)
    def test(self, method: Method, results: ncu.ProfilingResults) -> None:
        """
        If there are 2 sequences, twice :py:attr:`SECTOR_COUNT_LOAD` are requested,
        but half is served from the cache.
        """
        _, metrics = results.query_single_next((method.name.lower(),))

        match method:
            case Method.GLOBAL_KERNEL | Method.RESTRICT_RECAST_LAMBDA | Method.LDG_ACCESSOR:
                factor = 1
            case Method.RESTRICT_ACCESSOR | Method.RESTRICT_MEMBER | Method.RESTRICT_VIEW_MEMORY_TRAIT:
                factor = 2
            case Method.RESTRICT_RECAST_LOCAL:
                factor = {
                    'NVIDIA': 1,
                    'Clang':  2,
                }[self.toolchains['CUDA']['compiler']['id']]
            case _:
                raise ValueError(method)

        assert metrics['L1/TEX cache global load sectors.sum']  == factor * self.SECTOR_COUNT_LOAD
        assert metrics['L1/TEX cache global store sectors.sum'] == factor * self.SECTOR_COUNT_STORE

        assert metrics['L1/TEX cache global load sectors miss.sum']          == self.SECTOR_COUNT_LOAD
        assert metrics['lts__t_sectors_srcunit_tex_op_read_lookup_miss.sum'] == self.SECTOR_COUNT_LOAD
